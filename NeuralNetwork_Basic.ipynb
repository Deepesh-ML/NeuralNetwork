{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A basic neural network implementation for illustration. There are sophisticated algorithms like PyTorch, TensorFlow, etc. available out there, where you can just import a module and design a model with layers and nodes and activation functions, loss and accuracy metrics, etc. Here, we are just trying to understand how it works.\n",
        "\n",
        "This code implements just one input layer, one hidden layer and one output layer to perform binary classification using the sigmoid activation function."
      ],
      "metadata": {
        "id": "kYZMKAYfCn_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx8CIyveXQBa"
      },
      "outputs": [],
      "source": [
        "# start with importing the required library, NumPy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign input values consisting of four samples with two features each\n",
        "input_value = np.array([[0,0], [0,1], [1,1], [1,0]])\n",
        "print(input_value.shape)\n",
        "print(input_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9TRDaZJXUH3",
        "outputId": "356b5822-744b-4b22-babc-c14f7b361990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 2)\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign output values and reshape it to a column vector\n",
        "output = np.array([0,1,1,0])\n",
        "output = output.reshape(4,1)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCcJT52gXhV-",
        "outputId": "f0942112-99a4-4aa4-f299-098bd456af63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign weights\n",
        "# initial weights are randomly assigned and consist of 2 rows (1 for each input feature)\n",
        "# it consists of only one column because there's only one neuron in the hidden layer\n",
        "weights = np.array([[0.1], [0.2]])\n",
        "print(weights)\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNDA_6W2YF_W",
        "outputId": "02300bdf-183c-4f60-884f-062e7970db16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1]\n",
            " [0.2]]\n",
            "(2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add bias\n",
        "# the bias is initialized to 0.3\n",
        "bias = 0.3"
      ],
      "metadata": {
        "id": "p5MBtZBHYNHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function\n",
        "# it squashes the weighted sum of inputs to a value between 0 and 1\n",
        "def sigmoid_func(x):\n",
        "  return 1/(1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "88RX4yROYR_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derivative of sigmoid function\n",
        "# it is used in the gradient descent algorith during backpropagation\n",
        "def der(x):\n",
        "  return sigmoid_func(x) * (1 - sigmoid_func(x))"
      ],
      "metadata": {
        "id": "oEImQCTJYZyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # assign input values\n",
        "# input_value = np.array([[0,0], [0,1], [1,1], [1,0]])\n",
        "\n",
        "# # assign output values\n",
        "# output = np.array([0,1,1,0])\n",
        "# output = output.reshape(4,1)\n",
        "\n",
        "# # assign weights\n",
        "# weights = np.array([[0.1], [0.2]])\n",
        "\n",
        "# # add bias\n",
        "# bias = 0.3\n",
        "\n",
        "# # activation function\n",
        "# def sigmoid_func(x):\n",
        "#   return 1/(1 + np.exp(-x))\n",
        "\n",
        "# # derivative of sigmoid function\n",
        "# def der(x):\n",
        "#   return sigmoid_func(x) * (1 - sigmoid_func(x))\n",
        "\n",
        "# updating weights\n",
        "for epochs in range(10000): # repeat the training process 10000 times\n",
        "  input_arr = input_value\n",
        "\n",
        "  # get the dot product of features and weights and add a bias term\n",
        "  weighted_sum = np.dot(input_arr, weights) + bias\n",
        "  # this results in a first output of the hidden layer, which is the first feed forward\n",
        "  first_output = sigmoid_func(weighted_sum)\n",
        "\n",
        "  # compare the output with the original output and compute the error\n",
        "  error = first_output - output\n",
        "  # total error computed as a mean-squared error\n",
        "  total_error = np.square(np.subtract(first_output, output)).mean()\n",
        "\n",
        "  # compute the derivative to incorporate in the gradient descent algorithm\n",
        "  # the first derivative refers to the derivative of the loss function with\n",
        "  # respect to the output of the network. this provides us with the adjustment\n",
        "  # needed to reduce the loss. therefore, 'error' is used to represent this\n",
        "  # first derivative, which tells us how much the output needs to change to\n",
        "  # minimize the loss\n",
        "  first_der = error\n",
        "  # the second derivative is the derivative of the activation/sigmoid function\n",
        "  # with respect to its input. it is used to compute the rate of change of output\n",
        "  # with respect to the input.\n",
        "  second_der = der(first_output)\n",
        "  # multiply first and second derivatives for full derivative for each output.\n",
        "  # Multiplying the first and second derivatives together is a step in the\n",
        "  # backpropagation algorithm, specifically during the calculation of the\n",
        "  # gradient of the loss function with respect to the weights of the network.\n",
        "  # This operation is part of the chain rule, which allows us to compute the\n",
        "  # gradient of the loss function with respect to the weights by propagating\n",
        "  # the error backwards through the network.\n",
        "  derivative = first_der * second_der\n",
        "\n",
        "  # transpose of the input values to change it from (4,2) to (2,4)\n",
        "  t_input = input_value.T\n",
        "  # final derivative is the dot product of the input values\n",
        "  final_derivative = np.dot(t_input, derivative)\n",
        "\n",
        "  # update weights\n",
        "  # weights are updated by adding/subtracting the original weight with the product\n",
        "  # of learning rate and the derivative of weight function\n",
        "  weights = weights - 0.05 * final_derivative\n",
        "\n",
        "  # update bias\n",
        "  for i in derivative:\n",
        "    bias = bias - 0.05 * i\n",
        "\n",
        "print(weights)\n",
        "print(bias)\n",
        "\n",
        "# # prediction\n",
        "# # check the prediction for one of the input values [0,1]\n",
        "# pred = np.array([0, 1])\n",
        "# # keep checking for other values as well\n",
        "# # in two cases, it will result values very close to 1 and in the other two,\n",
        "# # values will be very close to zero.\n",
        "# result = np.dot(pred, weights) + bias\n",
        "# res = sigmoid_func(result)\n",
        "# print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E4du7jRYjW9",
        "outputId": "12b5f3b3-3f5b-4bf7-adf0-9f1083c78a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.41953547]\n",
            " [ 8.98887811]]\n",
            "[-4.19706344]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "# check the prediction for one of the input values [0,1]\n",
        "pred = np.array([0, 1])\n",
        "# keep checking for other values as well\n",
        "# in two cases, it will result values very close to 1 and in the other two,\n",
        "# values will be very close to zero.\n",
        "result = np.dot(pred, weights) + bias\n",
        "res = sigmoid_func(result)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg9Wy28gZXgv",
        "outputId": "aeb308b7-fb95-40ca-9481-ea9bb3003756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.99177089]\n"
          ]
        }
      ]
    }
  ]
}